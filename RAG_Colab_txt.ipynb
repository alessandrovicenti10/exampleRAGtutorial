{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandrovicenti10/exampleRAGtutorial/blob/main/RAG_Colab_txt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPChdPKEIDqA"
      },
      "source": [
        "# LLM RAG Tutorial\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SamHollings/llm_tutorial/blob/main/llm_tutorial_rag.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "This tutorial will give you a simple introduction to how to get started with an LLM to make a simple RAG app.\n",
        "\n",
        "RAG (Retrieval Augmented Generation) allows us to give foundational models local context, without doing expensive fine-tuning and can be done even normal everyday machines like your laptop.\n",
        "The basic idea is that we store documents as vectors in a database. When the user asks a question to the LLM, we can use langchain to first pass that question to the vector database, which retrieves relevant documents (these can be broken up into chunks, given metadata, summarised and various other steps to improve retrieval). The original question and these documents are then passed to the LLM (e.g. Claude) which then gives back the answer. So, in effect the model seems like it knows about what was in the database, e.g. local knowledge about your business, or hobby or whatever, whe in reality, that information was just injected into the prompt just prior to the model seeing it!\n",
        "\n",
        "The main libraries we will use are:\n",
        "- Langchain: which is basically a wrapper around the various LLMs and other tools to make it more consistent (so you can swap say.. OpenAI for Anthropic, easily)\n",
        "- Anthropic: which is the library through which we will access the Claude model (more on why this is chosen below)\n",
        "- ChromaDB: this is a simple vector database, which is a key part of the RAG model.\n",
        "- sentence-transformer: this is an open-source model for embedding text\n",
        "\n",
        "None of the above are \"the best\" tools - they're just examples, and you may whish to use difference embedding models, LLMs, vector databases, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZcfSPJkIDqC",
        "outputId": "ac118511-3716-4911-9cbb-f227cd19224e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on Colab\n",
            "/content/llm_tutorial\n"
          ]
        }
      ],
      "source": [
        "# this forces google collab to install the dependencies\n",
        "if \"google.colab\" in str(get_ipython()):\n",
        "    print(\"Running on Colab\")\n",
        "    !git clone https://github.com/SamHollings/llm_tutorial.git -q\n",
        "    %cd llm_tutorial\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q -q\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TMm50z7FIuc0",
        "outputId": "ceb764d5-542e-4203-d9b2-9dca708e2e5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.5/199.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.5 which is incompatible.\n",
            "notebook 6.5.5 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.6.3 which is incompatible.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ipykernel==5.5.6\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4sWBv8HNJU-5",
        "outputId": "289c7afb-106f-4c23-a4b4-b3bc0a15639c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 828
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipykernel==5.5.6\n",
            "  Downloading ipykernel-5.5.6-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6) (8.6.3)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel==5.5.6) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel==5.5.6) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel==5.5.6) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel==5.5.6) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel==5.5.6) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel==5.5.6) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel==5.5.6) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel==5.5.6) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel==5.5.6) (4.9.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel==5.5.6) (5.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel==5.5.6) (2.8.2)\n",
            "Requirement already satisfied: pyzmq>=23.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel==5.5.6) (24.0.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel==5.5.6) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-client->ipykernel==5.5.6) (4.3.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.0.0->ipykernel==5.5.6) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel==5.5.6) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->jupyter-client->ipykernel==5.5.6) (1.16.0)\n",
            "Downloading ipykernel-5.5.6-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ipykernel\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 6.29.5\n",
            "    Uninstalling ipykernel-6.29.5:\n",
            "      Successfully uninstalled ipykernel-6.29.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyterlab 4.3.3 requires ipykernel>=6.5.0, but you have ipykernel 5.5.6 which is incompatible.\n",
            "notebook 6.5.5 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ipykernel-5.5.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ipykernel"
                ]
              },
              "id": "81edda48611844c38a1d29643f8c29dc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zaKMGQT9tpdj",
        "outputId": "ce027743-1b8d-4ef1-c54f-ce2e52f6ec04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3FXEZS2oJAf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Configurazione\n",
        "DEV_MODE = True\n",
        "PERSIST_DIRECTORY = \"/content/llm_tutorial/db\"\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "if DEV_MODE:\n",
        "    PERSIST_DIRECTORY += \"/dev\"\n",
        "\n",
        "# Inizializza il modello di embedding\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
        "\n",
        "# Inizializza il client HuggingFace\n",
        "huggingface_pipeline = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Funzione per generare gli embedding\n",
        "def generate_embeddings(texts):\n",
        "    embeddings = embedding_model.encode(texts, convert_to_tensor=True)\n",
        "    return embeddings.cpu().numpy()  # Sposta su CPU e converte in NumPy\n",
        "\n",
        "# Salva gli embedding su file\n",
        "def save_embeddings(embeddings, metadata):\n",
        "    os.makedirs(PERSIST_DIRECTORY, exist_ok=True)\n",
        "    temp_file_path = os.path.join(PERSIST_DIRECTORY, \"embeddings_temp.npz\")\n",
        "    final_file_path = os.path.join(PERSIST_DIRECTORY, \"embeddings.npz\")\n",
        "    np.savez(temp_file_path, embeddings=embeddings, metadata=metadata)\n",
        "    os.rename(temp_file_path, final_file_path)  # Sostituisce il file solo se il salvataggio è riuscito\n",
        "    print(f\"File degli embeddings salvato in: {final_file_path}\")\n",
        "\n",
        "# Carica gli embedding da file\n",
        "def load_embeddings():\n",
        "    file_path = os.path.join(PERSIST_DIRECTORY, \"embeddings.npz\")\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Il file degli embedding '{file_path}' non esiste.\")\n",
        "    try:\n",
        "        data = np.load(file_path, allow_pickle=True)\n",
        "        return data[\"embeddings\"], data[\"metadata\"]\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Errore durante il caricamento di '{file_path}': {e}\")\n",
        "\n",
        "# Recupera i documenti più simili alla query\n",
        "def retrieve_documents(query, embeddings, metadata, k=4):\n",
        "    query_embedding = generate_embeddings([query])[0]  # Genera embedding per la query\n",
        "    distances = cdist([query_embedding], embeddings, metric=\"cosine\")[0]\n",
        "    indices = np.argsort(distances)[:k]\n",
        "    return [metadata[i] for i in indices]\n",
        "\n",
        "# Invia una domanda al modello HuggingFace\n",
        "def query_huggingface(prompt):\n",
        "    response = huggingface_pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=150,         # Numero massimo di nuovi token generati\n",
        "        num_return_sequences=1,    # Un'unica sequenza di ritorno\n",
        "        truncation=True,           # Attiva il troncamento per gli input troppo lunghi\n",
        "        pad_token_id=50256         # Imposta l'ID del token di padding (modifica se necessario per il tuo modello)\n",
        "    )\n",
        "    return response[0][\"generated_text\"]\n",
        "\n",
        "\n",
        "# Popolazione del database vettoriale\n",
        "def populate_vectorstore():\n",
        "    documents = []\n",
        "    docs_path = \"/content/llm_tutorial/docs/goldacre_review.txt\"\n",
        "\n",
        "    # Leggi il file specifico\n",
        "    if not os.path.exists(docs_path):\n",
        "        raise FileNotFoundError(f\"File '{docs_path}' non trovato.\")\n",
        "\n",
        "    with open(docs_path, \"r\", encoding=\"utf-8\") as text_file:\n",
        "        documents.append(text_file.read())\n",
        "\n",
        "    print(f\"Documenti caricati: {len(documents)}\")\n",
        "\n",
        "    # Genera embedding per i documenti\n",
        "    metadata = documents  # Metadata è semplicemente il testo originale\n",
        "    embeddings = generate_embeddings(documents)\n",
        "    print(f\"Embedding generati: {embeddings.shape}\")\n",
        "    save_embeddings(embeddings, metadata)\n",
        "\n",
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "    embeddings_file_path = os.path.join(PERSIST_DIRECTORY, \"embeddings.npz\")\n",
        "\n",
        "    # Controlla se il file degli embedding esiste ed è integro\n",
        "    if not os.path.exists(embeddings_file_path):\n",
        "        print(f\"File '{embeddings_file_path}' non trovato. Creazione del database vettoriale...\")\n",
        "        populate_vectorstore()\n",
        "    else:\n",
        "        print(f\"File '{embeddings_file_path}' trovato. Verifica integrità...\")\n",
        "\n",
        "        try:\n",
        "            embeddings, metadata = load_embeddings()\n",
        "        except (FileNotFoundError, ValueError) as e:\n",
        "            print(f\"Errore durante il caricamento del file: {e}\")\n",
        "            print(\"Ricreazione del database vettoriale...\")\n",
        "            populate_vectorstore()\n",
        "            embeddings, metadata = load_embeddings()\n",
        "\n",
        "    # Carica embedding e metadata\n",
        "    embeddings, metadata = load_embeddings()\n",
        "\n",
        "    # Domanda dell'utente\n",
        "    question = \"Describe what the Goldacre says about RAP (Reproducible Analytical Pipelines) and what we need to do to make them work.\"\n",
        "\n",
        "    # Recupera i documenti più rilevanti\n",
        "    relevant_docs = retrieve_documents(question, embeddings, metadata)\n",
        "\n",
        "    # Crea il prompt per HuggingFace con troncamento\n",
        "    max_prompt_length = 500  # Lunghezza massima del prompt\n",
        "    truncated_docs = \" \".join(relevant_docs)[:max_prompt_length]\n",
        "    prompt = f\"Using the following documents: {truncated_docs}\\nAnswer the question: {question}\"\n",
        "\n",
        "\n",
        "    # Ottieni la risposta\n",
        "    answer = query_huggingface(prompt)\n",
        "    print(\"Answer from HuggingFace:\", answer)\n"
      ],
      "metadata": {
        "id": "4_d1PbyEubJN",
        "outputId": "a29a378b-1482-4694-f8da-d5d5fbd4672e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File '/content/llm_tutorial/db/dev/embeddings.npz' trovato. Verifica integrità...\n",
            "Answer from HuggingFace: Using the following documents: Skip to main content\n",
            " GOV.UK\n",
            " Navigation menu \n",
            "Menu Search GOV.UK \n",
            "HomeHealth and social careTechnology in health and social careBetter, broader, safer: using health data for research and analysis\n",
            "Department\n",
            "of Health &\n",
            "Social Care\n",
            "Independent report\n",
            "Better, broader, safer: using health data for research and analysis\n",
            "Published 7 April 2022\n",
            "\n",
            "Applies to England\n",
            "Contents\n",
            "Review team\n",
            "Senior stakeholder group\n",
            "Background information\n",
            "Ministerial introduction\n",
            "Foreword\n",
            "Executive summary\n",
            "Summary recommend\n",
            "Answer the question: Describe what the Goldacre says about RAP (Reproducible Analytical Pipelines) and what we need to do to make them work.\n",
            "Key points\n",
            "\n",
            "What this report will cover\n",
            "\n",
            "\n",
            "Conduct the investigation\n",
            "\n",
            "\n",
            "Identify the risks\n",
            "\n",
            "Satisfy the team\n",
            "\n",
            "\n",
            "Develop solutions to the problem. Develop a plan to address the problem.\n",
            "\n",
            "Use research data\n",
            "\n",
            "\n",
            "Identify the costs\n",
            "\n",
            "\n",
            "Reach the team\n",
            "\n",
            "\n",
            "Advise how to implement, test and achieve the goals\n",
            "\n",
            "\n",
            "Document and publish\n",
            "\n",
            "\n",
            "Review process\n",
            "\n",
            "Background information\n",
            "\n",
            "Ministerial introduction\n",
            "\n",
            "Summary recommend\n",
            "\n",
            "Answer the first question above.\n",
            "\n",
            "Commentary\n",
            "\n",
            "\n",
            "Review\n",
            "\n",
            "Summary recommend\n",
            "\n",
            "Answer the second question above. Also review the report on the Goldacre and the first section on Health Technology in Health.\n",
            "\n",
            "Reach the project team\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBl-MxtFIDqF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ET91Mi2uS6gg",
        "outputId": "7d1cfd00-b5e6-4b79-96b7-6972f8b3dddd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}